{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":471627,"sourceType":"datasetVersion","datasetId":212391}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"003a4ec1-4849-4b95-9a46-beea59604564","cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n    # for filename in filenames:\n    #     print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:23:17.714579Z","iopub.execute_input":"2025-04-07T20:23:17.714871Z","iopub.status.idle":"2025-04-07T20:23:18.726879Z","shell.execute_reply.started":"2025-04-07T20:23:17.714840Z","shell.execute_reply":"2025-04-07T20:23:18.725656Z"}},"outputs":[],"execution_count":1},{"id":"14a21bb8-3aa7-44ef-8514-33b94561f279","cell_type":"code","source":"!pip install ipython pandas numpy seaborn librosa soundfile scipy torch matplotlib nnAudio torchaudio tqdm Levenshtein transformers scikit-learn -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:23:21.300531Z","iopub.execute_input":"2025-04-07T20:23:21.300971Z","iopub.status.idle":"2025-04-07T20:23:29.451095Z","shell.execute_reply.started":"2025-04-07T20:23:21.300940Z","shell.execute_reply":"2025-04-07T20:23:29.450091Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.5/161.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"id":"065dcaff-c05f-4bc5-8f4f-024ab993ef66","cell_type":"code","source":"import IPython.display as ipd\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport librosa\nimport soundfile as sf\nimport re\nimport scipy\nimport torch\nimport matplotlib.pyplot as plt\nimport nnAudio\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchaudio\nimport gc\nimport seaborn as sns\n\nfrom glob import glob\nfrom tqdm import tqdm\nfrom nnAudio.features.mel import MelSpectrogram\nfrom torchaudio.transforms import AmplitudeToDB\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC, \n    AutoProcessor, Wav2Vec2Model, HubertForCTC, \n    HubertModel, Data2VecAudioForCTC, Data2VecAudioModel,\n    AutoModelForAudioXVector\n)\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:23:29.452427Z","iopub.execute_input":"2025-04-07T20:23:29.452675Z","iopub.status.idle":"2025-04-07T20:23:53.168002Z","shell.execute_reply.started":"2025-04-07T20:23:29.452650Z","shell.execute_reply":"2025-04-07T20:23:53.167057Z"}},"outputs":[],"execution_count":3},{"id":"505a644e","cell_type":"code","source":"DEVICE = \"cuda\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:23:57.964638Z","iopub.execute_input":"2025-04-07T20:23:57.965280Z","iopub.status.idle":"2025-04-07T20:23:57.969337Z","shell.execute_reply.started":"2025-04-07T20:23:57.965248Z","shell.execute_reply":"2025-04-07T20:23:57.968235Z"}},"outputs":[],"execution_count":4},{"id":"24d77231-f9be-407a-a610-30eb860ec9d6","cell_type":"markdown","source":"<a id='Homework'></a>\n# Homework","metadata":{}},{"id":"6b5b390a-c130-420e-8666-aa083f943155","cell_type":"markdown","source":"Theory (8 points):\n- Follow links.\n- Try to fill/do **TODO** comments.\n- Answer theory questions in the Google Form.\n\nPractice (10 points):\n\n#### Option 1 - Phoneme Recognition (PR)\nImplement PR model using SSL features.\n\n1. Take [TIMIT Dataset](https://www.kaggle.com/datasets/mfekadu/darpa-timit-acousticphonetic-continuous-speech).\n2. Build model based on SSL features:\n    - Try several SSL models — you can take models from lecture or/and explore [HF Model Hub](https://huggingface.co/models) or/and explore [Superb Leaderboard](https://superbbenchmark.org/leaderboard).\n    - In lecture we have used `last_hidden_state`. Experiment with other layers, as an example check [Hubert/4 Experimental Details/C](https://arxiv.org/abs/2106.07447).\n    - Experiment with different \"Heads\": Linear, MLP, Recurrent (GRU, LSTM), Conv, Attention. DO NOT use \"too Deep\" heads.\n3. Use PER (Phonem Error Rate) for evaluation. Use pre-defined train/test split.\n\nExample: https://www.kaggle.com/code/vladimirsydor/phoneme-recognition-with-wav2vec2.\n\n#### Option 2 - Automatic Speech Recognition (ASR)\nImplement ASR modle using SSLF features.\n\n1. Take [OpenTTS UK dataset](https://huggingface.co/datasets/Yehor/opentts-uk)\n2. Check **Option 1 description**\n3. Evaluate model on [Common Voice 10 Test set](https://github.com/egorsmkv/cv10-uk-testset-clean). Use WER (Word Error Rate) and Character Error Rate (CER) for evaluation.","metadata":{}},{"id":"c49b9826-7463-4558-a3c6-e1f3edb0de24","cell_type":"markdown","source":"<a id='Optional_Additional_Task'></a>\n# Optional Additional Task\n\n**10 Points**\n\nVoice Conversion based on:\n- SSL features.\n- KNN (K Nearest Neighbors).\n- Vocoding from SSL features.\n\nResources:\n- [Code](https://arxiv.org/abs/2305.18975).\n- [Paper](https://github.com/bshall/knn-vc).\n\nApply the proposed approach using another SSL features (they have used WavLM):\n\n- Try 2-3 another SSL features and different layers.\n- Do Voice Conversion on VCTK.\n- Train Vocoder on LIBRISPEECH or/and VCTK.\n- Compare results with paper results. ","metadata":{}},{"id":"10aac4e8-2ac2-4d41-b34d-550725738e45","cell_type":"markdown","source":"## Prepare Data","metadata":{}},{"id":"17a5218a-7ed6-4a78-b161-ae7cf78de4d4","cell_type":"code","source":"import pandas as pd\nimport os\n\ntrain_data = pd.read_csv(\"../input/darpa-timit-acousticphonetic-continuous-speech/train_data.csv\").dropna()\ntest_data = pd.read_csv(\"../input/darpa-timit-acousticphonetic-continuous-speech/test_data.csv\").dropna()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:24:03.671167Z","iopub.execute_input":"2025-04-07T20:24:03.671562Z","iopub.status.idle":"2025-04-07T20:24:03.890398Z","shell.execute_reply.started":"2025-04-07T20:24:03.671532Z","shell.execute_reply":"2025-04-07T20:24:03.889305Z"}},"outputs":[],"execution_count":5},{"id":"596be821-605c-4b4a-8df7-cdd006133a9f","cell_type":"code","source":"train_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:24:04.173382Z","iopub.execute_input":"2025-04-07T20:24:04.173839Z","iopub.status.idle":"2025-04-07T20:24:04.222682Z","shell.execute_reply.started":"2025-04-07T20:24:04.173801Z","shell.execute_reply":"2025-04-07T20:24:04.220908Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   index test_or_train dialect_region speaker_id       filename  \\\n0    1.0         TRAIN            DR4      MMDM0  SI681.WAV.wav   \n1    2.0         TRAIN            DR4      MMDM0     SI1311.PHN   \n2    3.0         TRAIN            DR4      MMDM0     SI1311.WRD   \n3    4.0         TRAIN            DR4      MMDM0      SX321.PHN   \n4    5.0         TRAIN            DR4      MMDM0      SX321.WRD   \n\n              path_from_data_dir        path_from_data_dir_windows  \\\n0  TRAIN/DR4/MMDM0/SI681.WAV.wav  TRAIN\\\\DR4\\\\MMDM0\\\\SI681.WAV.wav   \n1     TRAIN/DR4/MMDM0/SI1311.PHN     TRAIN\\\\DR4\\\\MMDM0\\\\SI1311.PHN   \n2     TRAIN/DR4/MMDM0/SI1311.WRD     TRAIN\\\\DR4\\\\MMDM0\\\\SI1311.WRD   \n3      TRAIN/DR4/MMDM0/SX321.PHN      TRAIN\\\\DR4\\\\MMDM0\\\\SX321.PHN   \n4      TRAIN/DR4/MMDM0/SX321.WRD      TRAIN\\\\DR4\\\\MMDM0\\\\SX321.WRD   \n\n  is_converted_audio is_audio is_word_file is_phonetic_file is_sentence_file  \n0               True     True        False            False            False  \n1              False    False        False             True            False  \n2              False    False         True            False            False  \n3              False    False        False             True            False  \n4              False    False         True            False            False  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>test_or_train</th>\n      <th>dialect_region</th>\n      <th>speaker_id</th>\n      <th>filename</th>\n      <th>path_from_data_dir</th>\n      <th>path_from_data_dir_windows</th>\n      <th>is_converted_audio</th>\n      <th>is_audio</th>\n      <th>is_word_file</th>\n      <th>is_phonetic_file</th>\n      <th>is_sentence_file</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>TRAIN</td>\n      <td>DR4</td>\n      <td>MMDM0</td>\n      <td>SI681.WAV.wav</td>\n      <td>TRAIN/DR4/MMDM0/SI681.WAV.wav</td>\n      <td>TRAIN\\\\DR4\\\\MMDM0\\\\SI681.WAV.wav</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>TRAIN</td>\n      <td>DR4</td>\n      <td>MMDM0</td>\n      <td>SI1311.PHN</td>\n      <td>TRAIN/DR4/MMDM0/SI1311.PHN</td>\n      <td>TRAIN\\\\DR4\\\\MMDM0\\\\SI1311.PHN</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.0</td>\n      <td>TRAIN</td>\n      <td>DR4</td>\n      <td>MMDM0</td>\n      <td>SI1311.WRD</td>\n      <td>TRAIN/DR4/MMDM0/SI1311.WRD</td>\n      <td>TRAIN\\\\DR4\\\\MMDM0\\\\SI1311.WRD</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.0</td>\n      <td>TRAIN</td>\n      <td>DR4</td>\n      <td>MMDM0</td>\n      <td>SX321.PHN</td>\n      <td>TRAIN/DR4/MMDM0/SX321.PHN</td>\n      <td>TRAIN\\\\DR4\\\\MMDM0\\\\SX321.PHN</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>TRAIN</td>\n      <td>DR4</td>\n      <td>MMDM0</td>\n      <td>SX321.WRD</td>\n      <td>TRAIN/DR4/MMDM0/SX321.WRD</td>\n      <td>TRAIN\\\\DR4\\\\MMDM0\\\\SX321.WRD</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"id":"3ace1804-b7cd-4fde-95d2-e89b7efa5159","cell_type":"code","source":"data_path = '/kaggle/input/darpa-timit-acousticphonetic-continuous-speech/data/'\n\ndef combine_files(df):\n    data = {}\n\n    for idx, row in tqdm(df.iterrows()):\n        path = row['path_from_data_dir']\n        entry_id = path.split('.')[0]\n    \n        if entry_id not in data:\n            data[entry_id] = {}\n    \n        if row['is_audio'] is True:\n            data[entry_id]['audio_file'] = os.path.join(data_path, path)\n        elif row['is_word_file'] is True:\n            data[entry_id]['word_file'] = os.path.join(data_path, path)\n        elif row['is_phonetic_file'] is True:\n            data[entry_id]['phonetic_file'] = os.path.join(data_path, path)\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:24:05.924961Z","iopub.execute_input":"2025-04-07T20:24:05.925352Z","iopub.status.idle":"2025-04-07T20:24:05.931477Z","shell.execute_reply.started":"2025-04-07T20:24:05.925318Z","shell.execute_reply":"2025-04-07T20:24:05.930476Z"}},"outputs":[],"execution_count":7},{"id":"c2bdf154-5b26-4164-969d-332fca472565","cell_type":"code","source":"combined_train_data = combine_files(train_data)\ncombined_test_data = combine_files(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:24:07.116643Z","iopub.execute_input":"2025-04-07T20:24:07.116965Z","iopub.status.idle":"2025-04-07T20:24:08.023864Z","shell.execute_reply.started":"2025-04-07T20:24:07.116940Z","shell.execute_reply":"2025-04-07T20:24:08.022886Z"}},"outputs":[{"name":"stderr","text":"8400it [00:00, 18383.52it/s]\n8400it [00:00, 19210.05it/s]\n","output_type":"stream"}],"execution_count":8},{"id":"da0d01fc-9db2-41cf-b72f-2968b0456ba7","cell_type":"code","source":"train_df = pd.DataFrame.from_dict(combined_train_data, orient=\"index\")\ntest_df = pd.DataFrame.from_dict(combined_test_data, orient=\"index\")\n\nval_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:24:09.637870Z","iopub.execute_input":"2025-04-07T20:24:09.638258Z","iopub.status.idle":"2025-04-07T20:24:09.651964Z","shell.execute_reply.started":"2025-04-07T20:24:09.638217Z","shell.execute_reply":"2025-04-07T20:24:09.650718Z"}},"outputs":[],"execution_count":9},{"id":"5bece824-9911-4550-b607-f8f25aef7b21","cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:24:11.272931Z","iopub.execute_input":"2025-04-07T20:24:11.273322Z","iopub.status.idle":"2025-04-07T20:24:11.283136Z","shell.execute_reply.started":"2025-04-07T20:24:11.273291Z","shell.execute_reply":"2025-04-07T20:24:11.282088Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                                               audio_file  \\\nTRAIN/DR4/MMDM0/SI681   /kaggle/input/darpa-timit-acousticphonetic-con...   \nTRAIN/DR4/MMDM0/SI1311  /kaggle/input/darpa-timit-acousticphonetic-con...   \nTRAIN/DR4/MMDM0/SX321   /kaggle/input/darpa-timit-acousticphonetic-con...   \nTRAIN/DR4/MMDM0/SX51    /kaggle/input/darpa-timit-acousticphonetic-con...   \nTRAIN/DR4/MMDM0/SX231   /kaggle/input/darpa-timit-acousticphonetic-con...   \n\n                                                            phonetic_file  \\\nTRAIN/DR4/MMDM0/SI681   /kaggle/input/darpa-timit-acousticphonetic-con...   \nTRAIN/DR4/MMDM0/SI1311  /kaggle/input/darpa-timit-acousticphonetic-con...   \nTRAIN/DR4/MMDM0/SX321   /kaggle/input/darpa-timit-acousticphonetic-con...   \nTRAIN/DR4/MMDM0/SX51    /kaggle/input/darpa-timit-acousticphonetic-con...   \nTRAIN/DR4/MMDM0/SX231   /kaggle/input/darpa-timit-acousticphonetic-con...   \n\n                                                                word_file  \nTRAIN/DR4/MMDM0/SI681   /kaggle/input/darpa-timit-acousticphonetic-con...  \nTRAIN/DR4/MMDM0/SI1311  /kaggle/input/darpa-timit-acousticphonetic-con...  \nTRAIN/DR4/MMDM0/SX321   /kaggle/input/darpa-timit-acousticphonetic-con...  \nTRAIN/DR4/MMDM0/SX51    /kaggle/input/darpa-timit-acousticphonetic-con...  \nTRAIN/DR4/MMDM0/SX231   /kaggle/input/darpa-timit-acousticphonetic-con...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>audio_file</th>\n      <th>phonetic_file</th>\n      <th>word_file</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>TRAIN/DR4/MMDM0/SI681</th>\n      <td>/kaggle/input/darpa-timit-acousticphonetic-con...</td>\n      <td>/kaggle/input/darpa-timit-acousticphonetic-con...</td>\n      <td>/kaggle/input/darpa-timit-acousticphonetic-con...</td>\n    </tr>\n    <tr>\n      <th>TRAIN/DR4/MMDM0/SI1311</th>\n      <td>/kaggle/input/darpa-timit-acousticphonetic-con...</td>\n      <td>/kaggle/input/darpa-timit-acousticphonetic-con...</td>\n      <td>/kaggle/input/darpa-timit-acousticphonetic-con...</td>\n    </tr>\n    <tr>\n      <th>TRAIN/DR4/MMDM0/SX321</th>\n      <td>/kaggle/input/darpa-timit-acousticphonetic-con...</td>\n      <td>/kaggle/input/darpa-timit-acousticphonetic-con...</td>\n      <td>/kaggle/input/darpa-timit-acousticphonetic-con...</td>\n    </tr>\n    <tr>\n      <th>TRAIN/DR4/MMDM0/SX51</th>\n      <td>/kaggle/input/darpa-timit-acousticphonetic-con...</td>\n      <td>/kaggle/input/darpa-timit-acousticphonetic-con...</td>\n      <td>/kaggle/input/darpa-timit-acousticphonetic-con...</td>\n    </tr>\n    <tr>\n      <th>TRAIN/DR4/MMDM0/SX231</th>\n      <td>/kaggle/input/darpa-timit-acousticphonetic-con...</td>\n      <td>/kaggle/input/darpa-timit-acousticphonetic-con...</td>\n      <td>/kaggle/input/darpa-timit-acousticphonetic-con...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"id":"2a2e4052-2f9a-4b4c-8ebd-c5143f4a1c10","cell_type":"code","source":"train_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:24:12.374356Z","iopub.execute_input":"2025-04-07T20:24:12.374722Z","iopub.status.idle":"2025-04-07T20:24:12.380157Z","shell.execute_reply.started":"2025-04-07T20:24:12.374691Z","shell.execute_reply":"2025-04-07T20:24:12.379275Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(1680, 3)"},"metadata":{}}],"execution_count":11},{"id":"4ce34500-d713-435e-ae50-9d5d51ca4812","cell_type":"code","source":"phon61_map39 = {\n    'iy':'iy',  'ih':'ih',   'eh':'eh',  'ae':'ae',    'ix':'ih',  'ax':'ah',   'ah':'ah',  'uw':'uw',\n    'ux':'uw',  'uh':'uh',   'ao':'aa',  'aa':'aa',    'ey':'ey',  'ay':'ay',   'oy':'oy',  'aw':'aw',\n    'ow':'ow',  'l':'l',     'el':'l',  'r':'r',      'y':'y',    'w':'w',     'er':'er',  'axr':'er',\n    'm':'m',    'em':'m',     'n':'n',    'nx':'n',     'en':'n',  'ng':'ng',   'eng':'ng', 'ch':'ch',\n    'jh':'jh',  'dh':'dh',   'b':'b',    'd':'d',      'dx':'dx',  'g':'g',     'p':'p',    't':'t',\n    'k':'k',    'z':'z',     'zh':'sh',  'v':'v',      'f':'f',    'th':'th',   's':'s',    'sh':'sh',\n    'hh':'hh',  'hv':'hh',   'pcl':'h#', 'tcl':'h#', 'kcl':'h#', 'qcl':'h#','bcl':'h#','dcl':'h#',\n    'gcl':'h#','h#':'h#',  '#h':'h#',  'pau':'h#', 'epi': 'h#','nx':'n',   'ax-h':'ah','q':'h#' \n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:24:13.383703Z","iopub.execute_input":"2025-04-07T20:24:13.384010Z","iopub.status.idle":"2025-04-07T20:24:13.390474Z","shell.execute_reply.started":"2025-04-07T20:24:13.383986Z","shell.execute_reply":"2025-04-07T20:24:13.389195Z"}},"outputs":[],"execution_count":12},{"id":"5c02ab9f-1982-4ce9-9a40-1dc164913487","cell_type":"code","source":"def extract_file_data(df):\n    data = []\n    for i, row in df.iterrows():\n        waveform, _ = torchaudio.load(row[\"audio_file\"])\n        phonetic_transcription_list = open(row[\"phonetic_file\"]).readlines()\n        word_transcription_list = open(row[\"word_file\"]).readlines()\n\n        phonetic_transcription = \"\"\n        for phonetic_transcription_item in phonetic_transcription_list:\n            phonetic_transcription += phon61_map39[phonetic_transcription_item.split(\" \")[2].strip()] + \" \"\n\n        word_transcription = \"\"\n        for word_transcription_item in word_transcription_list:\n            word_transcription += word_transcription_item.split(\" \")[2].strip() + \" \"\n\n        data.append([waveform, phonetic_transcription, word_transcription])\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:24:13.712704Z","iopub.execute_input":"2025-04-07T20:24:13.713032Z","iopub.status.idle":"2025-04-07T20:24:13.718693Z","shell.execute_reply.started":"2025-04-07T20:24:13.713004Z","shell.execute_reply":"2025-04-07T20:24:13.717731Z"}},"outputs":[],"execution_count":13},{"id":"df3c7d12-aa15-4889-9b59-e988dcf461cd","cell_type":"code","source":"train_data = extract_file_data(train_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:24:15.654235Z","iopub.execute_input":"2025-04-07T20:24:15.654587Z","iopub.status.idle":"2025-04-07T20:25:00.596694Z","shell.execute_reply.started":"2025-04-07T20:24:15.654557Z","shell.execute_reply":"2025-04-07T20:25:00.595786Z"}},"outputs":[],"execution_count":14},{"id":"1581b8c5-f354-474f-8ccb-273dcf85e886","cell_type":"code","source":"test_data = extract_file_data(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:25:00.597867Z","iopub.execute_input":"2025-04-07T20:25:00.598104Z","iopub.status.idle":"2025-04-07T20:25:21.662811Z","shell.execute_reply.started":"2025-04-07T20:25:00.598082Z","shell.execute_reply":"2025-04-07T20:25:21.661685Z"}},"outputs":[],"execution_count":15},{"id":"20f55c54-27ab-4148-aab5-932cb27b2113","cell_type":"code","source":"val_data = extract_file_data(val_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:25:21.664522Z","iopub.execute_input":"2025-04-07T20:25:21.664891Z","iopub.status.idle":"2025-04-07T20:25:43.203040Z","shell.execute_reply.started":"2025-04-07T20:25:21.664853Z","shell.execute_reply":"2025-04-07T20:25:43.202062Z"}},"outputs":[],"execution_count":16},{"id":"f2216e72-85b2-4911-ac93-c15800d8c00d","cell_type":"code","source":"train_data[10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:25:43.204058Z","iopub.execute_input":"2025-04-07T20:25:43.204348Z","iopub.status.idle":"2025-04-07T20:25:43.261144Z","shell.execute_reply.started":"2025-04-07T20:25:43.204322Z","shell.execute_reply":"2025-04-07T20:25:43.260312Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"[tensor([[ 0.0000e+00, -3.0518e-04, -1.8311e-04,  ...,  9.1553e-05,\n           6.1035e-05,  6.1035e-05]]),\n 'h# g ih dx ih h# k ae l ih h# k ow h# k ae h# t ah h# k iy h# p h# ',\n 'get a calico cat to keep ']"},"metadata":{}}],"execution_count":17},{"id":"aaa1932d-fb59-4a5a-abf3-76324513b9c0","cell_type":"code","source":"phonetics = sorted(set([phone for x in train_data for phone in x[1].split()]))\nph2idx = {p: i for i, p in enumerate(phonetics)}\nidx2ph = {i: p for i, p in ph2idx.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:25:43.262741Z","iopub.execute_input":"2025-04-07T20:25:43.263055Z","iopub.status.idle":"2025-04-07T20:25:43.276395Z","shell.execute_reply.started":"2025-04-07T20:25:43.263028Z","shell.execute_reply":"2025-04-07T20:25:43.275241Z"}},"outputs":[],"execution_count":18},{"id":"2f85c9d0-b72a-47ad-b9d8-c74d75ff2e34","cell_type":"code","source":"def conv_ph2idx(data):\n    for row in data:\n        row[1] = [ph2idx[ph] for ph in row[1].split()]\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:25:46.701369Z","iopub.execute_input":"2025-04-07T20:25:46.701692Z","iopub.status.idle":"2025-04-07T20:25:46.706133Z","shell.execute_reply.started":"2025-04-07T20:25:46.701667Z","shell.execute_reply":"2025-04-07T20:25:46.705045Z"}},"outputs":[],"execution_count":20},{"id":"c490c038-bd65-487a-9f83-61f025843f7a","cell_type":"code","source":"train_data = conv_ph2idx(train_data)\ntest_data = conv_ph2idx(test_data)\nval_data = conv_ph2idx(val_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:25:47.155148Z","iopub.execute_input":"2025-04-07T20:25:47.155512Z","iopub.status.idle":"2025-04-07T20:25:47.181334Z","shell.execute_reply.started":"2025-04-07T20:25:47.155485Z","shell.execute_reply":"2025-04-07T20:25:47.180046Z"}},"outputs":[],"execution_count":21},{"id":"e1b719a0-494a-49bc-8cb3-562dc7fa87ba","cell_type":"code","source":"\nclass PhoneticDataset(Dataset):\n    def __init__(self, data, num_phoneme_classes):\n        \"\"\"\n        Initialize the dataset with multi-label phonetic encoding\n        \n        Args:\n            data (list): List of [waveform, phonetic_transcription, word_transcription]\n            num_phoneme_classes (int): Total number of unique phoneme classes\n        \"\"\"\n        self.data = data\n        self.num_phoneme_classes = num_phoneme_classes\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        Convert phonetic transcription to multi-label encoding\n        \n        Returns:\n            tuple: (waveform, multi_label_phonetic_encoding, word_transcription)\n        \"\"\"\n        waveform, phonetic_transcription, word_transcription = self.data[idx]\n    \n        \n        return waveform, phonetic_transcription, word_transcription","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:25:48.605665Z","iopub.execute_input":"2025-04-07T20:25:48.606004Z","iopub.status.idle":"2025-04-07T20:25:48.611735Z","shell.execute_reply.started":"2025-04-07T20:25:48.605977Z","shell.execute_reply":"2025-04-07T20:25:48.610375Z"}},"outputs":[],"execution_count":22},{"id":"855f9668-e6c4-4f4e-a155-f8cee02bc884","cell_type":"code","source":"train_ds = PhoneticDataset(train_data, len(phonetics))\ntest_ds = PhoneticDataset(test_data, len(phonetics))\nval_ds = PhoneticDataset(val_data, len(phonetics))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:25:48.896453Z","iopub.execute_input":"2025-04-07T20:25:48.896891Z","iopub.status.idle":"2025-04-07T20:25:48.902595Z","shell.execute_reply.started":"2025-04-07T20:25:48.896853Z","shell.execute_reply":"2025-04-07T20:25:48.901364Z"}},"outputs":[],"execution_count":23},{"id":"535030b3-93ab-426e-9ee9-723dc8b4ad31","cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\n\ndef collate_fn(batch):\n    # Unpack the batch\n    waveforms, phonetic_labels, word_transcriptions = zip(*batch)\n    \n    # --- Audio Padding ---\n    max_audio_length = max(waveform.shape[1] for waveform in waveforms)\n    padded_audio = []\n    attention_masks = []\n    \n    for waveform in waveforms:\n        # Pad audio to max length\n        pad_length = max_audio_length - waveform.shape[1]\n        padded = torch.nn.functional.pad(waveform, (0, pad_length))\n        \n        # Create attention mask (1 for real audio, 0 for padding)\n        mask = torch.ones_like(waveform, dtype=torch.long)\n        mask = torch.nn.functional.pad(mask, (0, pad_length))\n        \n        padded_audio.append(padded)\n        attention_masks.append(mask)\n    \n    # Stack audio and masks\n    padded_audio = torch.stack(padded_audio)\n    attention_masks = torch.stack(attention_masks)\n    \n    # --- Phonetic Label Padding ---\n    label_lengths = torch.tensor([len(labels) for labels in phonetic_labels])\n    padded_labels = pad_sequence(\n        [torch.tensor(labels) for labels in phonetic_labels],\n        batch_first=True,\n        padding_value=-100\n    )\n    \n    return padded_audio, attention_masks, padded_labels, label_lengths, word_transcriptions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:25:50.108439Z","iopub.execute_input":"2025-04-07T20:25:50.108799Z","iopub.status.idle":"2025-04-07T20:25:50.116454Z","shell.execute_reply.started":"2025-04-07T20:25:50.108768Z","shell.execute_reply":"2025-04-07T20:25:50.115147Z"}},"outputs":[],"execution_count":24},{"id":"16bee50b-d54c-4338-be87-46d5990d2b8e","cell_type":"code","source":"batch_size = 4\n\ntrain_dataloader = DataLoader(\n    train_ds, \n    batch_size=batch_size, \n    shuffle=True, \n    collate_fn=collate_fn,\n    num_workers=3\n)\n\ntest_dataloader = DataLoader(\n    test_ds, \n    batch_size=batch_size, \n    collate_fn=collate_fn,\n    num_workers=3\n)\n\nval_dataloader = DataLoader(\n    val_ds, \n    batch_size=batch_size, \n    collate_fn=collate_fn,\n    num_workers=3\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:25:51.016895Z","iopub.execute_input":"2025-04-07T20:25:51.017323Z","iopub.status.idle":"2025-04-07T20:25:51.023009Z","shell.execute_reply.started":"2025-04-07T20:25:51.017288Z","shell.execute_reply":"2025-04-07T20:25:51.021831Z"}},"outputs":[],"execution_count":25},{"id":"750c7efc-f15a-497e-9859-6d1288a429bd","cell_type":"code","source":"for batch_idx, (audio_inputs, attention_masks, phonetic_labels,label_lengths, word_transcriptions) in enumerate(train_dataloader):\n    audio_inputs = audio_inputs.to(DEVICE)\n    attention_masks = attention_masks.to(DEVICE)\n    print(phonetic_labels)\n    phonetic_labels = phonetic_labels.to(DEVICE)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:25:52.534304Z","iopub.execute_input":"2025-04-07T20:25:52.534655Z","iopub.status.idle":"2025-04-07T20:25:53.075045Z","shell.execute_reply.started":"2025-04-07T20:25:52.534626Z","shell.execute_reply":"2025-04-07T20:25:53.073635Z"}},"outputs":[{"name":"stdout","text":"tensor([[  15,    8,   17,   15,    7,   28,    2,   24,   15,   20,   11,    9,\n           17,   38,   17,   29,   25,   30,   21,   15,    3,   15,   20,    1,\n           29,   15,   31,   15, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100],\n        [  15,    8,   18,   25,   35,   11,   36,   12,   15,    6,    0,   28,\n           22,   11,   15,   20,   33,   15,    7,   29,   21,   17,   15,   27,\n           26,   38,   23,   17,   23,   15,   31,   15,   10,   23,   18,   36,\n            2,   23,   38,   15,   31,   18,   15],\n        [  15,    8,   17,   15,   14,    2,   35,   11,   22,   17,   23,   15,\n           29,    0,   15,    0,   32,   11,   17,   38,   12,   30,   17,   23,\n           15,    2,   35,   16,   17,   29,   17,    9,   17,   29,   23,   30,\n           17,   15,   15, -100, -100, -100, -100],\n        [  15,    5,   28,    4,   15,   29,    2,   23,   30,    4,   23,   30,\n           17,   22,   11,   38,    0,   23,    8,   18,   37,   25,   30,   23,\n           15, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100]])\n","output_type":"stream"}],"execution_count":26},{"id":"09c2c506-1dd6-4b8f-a069-76c16591665e","cell_type":"markdown","source":"## Test Wav2Vec","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"id":"ab55e218-0c8c-4f5a-bccd-e49cd7781d14","cell_type":"code","source":"model = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\").to(DEVICE)\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a6e25746-49ca-4d1e-b006-1171555f7986","cell_type":"code","source":"# Refactored from https://chat.openai.com/share/e/52f6acc9-152d-4a99-b874-86998ad8fcc7\ndef load_and_process_audio(paths, sample_rate=None):\n    # Load and resample audio files\n    audios = []\n    max_length = 0\n    for path in tqdm(paths):\n        audio, sr = librosa.load(path, sr=sample_rate)\n        max_length = max(max_length, len(audio))\n        audios.append(audio)\n\n    # Pad audios to have the same length and stack them into a batch\n    batched_audio = torch.zeros(len(audios), max_length)  # Assuming mono audio files\n    padding_masks = torch.zeros(len(audios), max_length)\n    for i, audio in enumerate(audios):\n        length = len(audio)\n        batched_audio[i, :length] = torch.from_numpy(audio)\n        padding_masks[i, :length] = 1  # Mark non-padded areas as 1\n\n    return batched_audio, padding_masks\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0dc2b22b-fb78-4df0-bc0f-95b1d1849144","cell_type":"code","source":"example_path = train_df.iloc[0]['audio_file']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c9fffb0b-2348-4dc9-af0a-ea8224f20401","cell_type":"code","source":"speaker_audio, speaker_pad_mask = load_and_process_audio([example_path])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f5df509f-b10b-44ff-a18a-f1050cf92a04","cell_type":"code","source":"ipd.Audio(speaker_audio, rate = 16000)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"aae1a396-b2f3-4e71-8f2c-eb0c089312cb","cell_type":"code","source":"input_values = feature_extractor([\n    el for el in speaker_audio.numpy()\n], return_tensors=\"pt\").input_values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8a2ae736-d859-4a93-b638-2f2eb00f95a8","cell_type":"code","source":"# Apply Model\nwith torch.no_grad():\n    model_output = model(input_values.cuda())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"81d6cfeb-e09b-478d-97e8-597541ee6686","cell_type":"code","source":"print(f\"Input Shape [B, L]: {input_values.shape}\")\nprint(f\"Ouput Shape [B, N_L, C]: {model_output.logits.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f4809220-d827-4509-b521-68e1c2222cd4","cell_type":"code","source":"pred_ids = torch.argmax(model_output.logits[0], axis=-1)\noutputs = tokenizer.decode(pred_ids, output_word_offsets=True)\noutputs.text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9b360edf-fb96-4334-b304-a037e9de66a9","cell_type":"markdown","source":"## Model","metadata":{}},{"id":"d864a841-03be-4ef4-a16e-558b60c896b1","cell_type":"markdown","source":"This model uses a frozen Wav2Vec 2.0 encoder (facebook/wav2vec2‑base‑960h) to turn raw audio into frame‑level feature vectors, then attaches a small, trainable head that maps those features to phoneme logits. \n\nYou can choose a simple two‑layer MLP (halving the hidden size, ReLU + dropout, then projecting to your phoneme set) or an LSTM stack (for extra temporal context) before the final linear layer.\n\nThe head outputs log‑probabilities in the shape required by nn.CTCLoss(blank=0), so you don’t need frame‑level alignments. By freezing the Wav2Vec backbone you only learn a few hundred thousand head parameters, keeping training fast and stable.","metadata":{}},{"id":"36550345-91b1-4320-9600-6360c883592f","cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import Wav2Vec2Model\n\nclass PhoneticRecognition(nn.Module):\n    def __init__(self, num_phonemes, pretrained_model='facebook/wav2vec2-base-960h', use_lstm=False, lstm_hidden_size=512, num_layers=2):\n        \"\"\"\n        Initialize Wav2Vec Phonetic Recognition Model with optional LSTM layer\n        \n        Args:\n            num_phonemes (int): Number of unique phonemes in the target dataset\n            pretrained_model (str): Pretrained wav2vec model to use as feature extractor\n            use_lstm (bool): Whether to use LSTM or MLP for classification\n            lstm_hidden_size (int): Hidden size for the LSTM (if use_lstm=True)\n            num_layers (int): Number of LSTM layers (if use_lstm=True)\n        \"\"\"\n        super(PhoneticRecognition, self).__init__()\n        \n        self.wav2vec = Wav2Vec2Model.from_pretrained(pretrained_model, output_hidden_states=True)\n        \n        for param in self.wav2vec.parameters():\n            param.requires_grad = False\n        \n        feature_dim = self.wav2vec.config.hidden_size\n        \n        self.use_lstm = use_lstm\n        \n        if self.use_lstm:\n            self.lstm = nn.LSTM(feature_dim, lstm_hidden_size, num_layers=num_layers, batch_first=False, dropout=0.5)\n            self.classifier = nn.Linear(lstm_hidden_size, num_phonemes)\n        else:\n            self.classifier = nn.Sequential(\n                nn.Linear(feature_dim, feature_dim // 2),\n                nn.ReLU(),\n                nn.Dropout(0.5),\n                nn.Linear(feature_dim // 2, num_phonemes)\n            )\n        \n    def forward(self, audio_input, attention_mask=None):\n        \"\"\"\n        Forward pass for phonetic recognition\n        \n        Args:\n            audio_input (torch.Tensor): Raw audio waveform\n            attention_mask (torch.Tensor, optional): Mask for padded sequences\n        \n        Returns:\n            torch.Tensor: Phoneme classification logits\n        \"\"\"\n        audio_input = audio_input.squeeze()\n        attention_mask = attention_mask.squeeze()\n\n        outputs = self.wav2vec(audio_input, attention_mask=attention_mask)\n        hidden_states = outputs.hidden_states\n\n        # tried some indices, 9 works better\n        features = hidden_states[9]\n\n        if self.use_lstm:\n            lstm_out, _ = self.lstm(features)\n            logits = self.classifier(lstm_out)  # (batch, seq_len, num_phonemes)\n        else:\n            logits = self.classifier(features)  # (batch, seq_len, num_phonemes)\n\n        log_probs = torch.log_softmax(logits, dim=-1)\n        return log_probs.permute(1, 0, 2)  # (seq_len, batch, num_phonemes)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:25:55.464281Z","iopub.execute_input":"2025-04-07T20:25:55.464735Z","iopub.status.idle":"2025-04-07T20:25:55.477400Z","shell.execute_reply.started":"2025-04-07T20:25:55.464690Z","shell.execute_reply":"2025-04-07T20:25:55.476133Z"}},"outputs":[],"execution_count":27},{"id":"4f718652-7630-47d5-854a-f153b27c8cf3","cell_type":"code","source":"import torch\nimport pytorch_lightning as pl\n\n\nclass PhoneticRecognitionModel(pl.LightningModule):\n    def __init__(self, model, learning_rate=1e-3):\n        super().__init__()\n        self.model = model \n        self.learning_rate = learning_rate\n        self.criterion = torch.nn.CTCLoss(blank=0)\n        \n    def forward(self, audio_inputs, attention_masks):\n        return self.model(audio_inputs, attention_masks)\n    \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n    \n    def _compute_loss(self, batch):\n        audio_inputs, attention_masks, phonetic_labels, phonetic_lengths, word_transcriptions = batch\n        outputs = self(audio_inputs, attention_masks)\n        \n        batch_size = audio_inputs.shape[0]\n        input_lengths = torch.full((batch_size,), outputs.shape[0])\n        \n        loss = self.criterion(outputs, phonetic_labels, input_lengths, phonetic_lengths)\n        return loss\n    \n    def training_step(self, batch, batch_idx):\n        loss = self._compute_loss(batch)\n        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        loss = self._compute_loss(batch)\n        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n        return loss\n    \n    def test_step(self, batch, batch_idx):\n        loss = self._compute_loss(batch)\n        self.log('test_loss', loss)\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:25:57.908129Z","iopub.execute_input":"2025-04-07T20:25:57.908509Z","iopub.status.idle":"2025-04-07T20:25:59.296587Z","shell.execute_reply.started":"2025-04-07T20:25:57.908480Z","shell.execute_reply":"2025-04-07T20:25:59.295733Z"}},"outputs":[],"execution_count":28},{"id":"cd380036-b146-40b4-8db8-822cec630d87","cell_type":"code","source":"pr_model_mlp = PhoneticRecognition(len(phonetics), use_lstm=False).to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:25:59.297747Z","iopub.execute_input":"2025-04-07T20:25:59.298012Z","iopub.status.idle":"2025-04-07T20:26:03.173643Z","shell.execute_reply.started":"2025-04-07T20:25:59.297988Z","shell.execute_reply":"2025-04-07T20:26:03.172711Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4bb44dd90fb42d7abb95df7bb02ccf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7a6932526bf4bea927e930529af4a81"}},"metadata":{}},{"name":"stderr","text":"Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":29},{"id":"1a885bb9-8d4b-4752-a04c-b302ec5ae09a","cell_type":"code","source":"from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nfrom pytorch_lightning.loggers import TensorBoardLogger\n\nlightning_model_mlp = PhoneticRecognitionModel(pr_model_mlp)\n\ncheckpoint_callback = ModelCheckpoint(\n    monitor='val_loss',\n    dirpath='checkpoints/',\n    filename='phonetic-recognition-{epoch:02d}-{val_loss:.4f}',\n    save_top_k=1,\n    mode='min'\n)\n\nlogger = TensorBoardLogger(\"lightning_logs\", name=\"phonetic_recognition\")\n\ntrainer = pl.Trainer(\n    max_epochs=10,\n    callbacks=[checkpoint_callback],\n    logger=logger,\n    accelerator='cuda' if torch.cuda.is_available() else 'cpu',\n    devices=1\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:33:02.324621Z","iopub.execute_input":"2025-04-07T20:33:02.324950Z","iopub.status.idle":"2025-04-07T20:33:02.382444Z","shell.execute_reply.started":"2025-04-07T20:33:02.324923Z","shell.execute_reply":"2025-04-07T20:33:02.381600Z"}},"outputs":[],"execution_count":38},{"id":"7a75c09e-3df8-4fda-a0fb-e6159ab49904","cell_type":"code","source":"trainer.fit(lightning_model_mlp, train_dataloader, val_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:26:08.205290Z","iopub.execute_input":"2025-04-07T20:26:08.205647Z","iopub.status.idle":"2025-04-07T20:32:28.435058Z","shell.execute_reply.started":"2025-04-07T20:26:08.205618Z","shell.execute_reply":"2025-04-07T20:32:28.434292Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /kaggle/working/checkpoints exists and is not empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffeb09bda0904710bcd079106438f356"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}],"execution_count":31},{"id":"3c1262e3-fa3b-4706-9776-65a5ece367af","cell_type":"code","source":"from typing import List\n\ndef edit_distance(a: List[int], b: List[int]) -> int:\n    # classic Wagner–Fischer\n    m, n = len(a), len(b)\n    dp = [[0]*(n+1) for _ in range(m+1)]\n    for i in range(m+1): dp[i][0] = i\n    for j in range(n+1): dp[0][j] = j\n\n    for i in range(1, m+1):\n        for j in range(1, n+1):\n            cost = 0 if a[i-1] == b[j-1] else 1\n            dp[i][j] = min(\n                dp[i-1][j] + 1,    # deletion\n                dp[i][j-1] + 1,    # insertion\n                dp[i-1][j-1] + cost  # substitution\n            )\n    return dp[m][n]\n\ndef calculate_per(hypothesis, reference):\n    if not reference:\n        return 0.0\n    return edit_distance(hypothesis, reference) / len(reference)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:32:31.621922Z","iopub.execute_input":"2025-04-07T20:32:31.622281Z","iopub.status.idle":"2025-04-07T20:32:31.629784Z","shell.execute_reply.started":"2025-04-07T20:32:31.622251Z","shell.execute_reply":"2025-04-07T20:32:31.628715Z"}},"outputs":[],"execution_count":32},{"id":"811830ae-13f1-4901-81f6-70dd4633cc8c","cell_type":"code","source":"def greedy_decode(log_probs, blank=0):\n    preds = log_probs.argmax(dim=-1).transpose(0,1)\n    decoded = []\n    for seq in preds:\n        tokens = []\n        prev = None\n        for t in seq.tolist():\n            if t != prev and t != blank:\n                tokens.append(t)\n            prev = t\n        decoded.append(tokens)\n    return decoded\n\ndef evaluate_model(model, test_dataloader):\n    model.eval()\n    total_per = 0.0\n    total_samples = 0\n\n    with torch.no_grad():\n        for audio, attn_mask, phonemes, phoneme_lens, _ in test_dataloader:\n            audio = audio.to(DEVICE)\n            attn_mask = attn_mask.to(DEVICE)\n\n            log_probs = model(audio, attn_mask)\n            hyps = greedy_decode(log_probs, blank=0)\n\n            refs = []\n            \n            for i in range(phonemes.shape[0]):\n                ref = phonemes[i].tolist()[:phoneme_lens[i]]\n                refs.append(ref)\n\n            # accumulate PER\n            for hyp, ref in zip(hyps, refs):\n                total_per += calculate_per(hyp, ref)\n                total_samples += 1\n\n\n    avg_per = total_per / total_samples if total_samples > 0 else 0.0\n    print(f\"PER on test dataset: {avg_per*100:.2f}%\")\n    return avg_per\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:32:33.313865Z","iopub.execute_input":"2025-04-07T20:32:33.314177Z","iopub.status.idle":"2025-04-07T20:32:33.322427Z","shell.execute_reply.started":"2025-04-07T20:32:33.314152Z","shell.execute_reply":"2025-04-07T20:32:33.321077Z"}},"outputs":[],"execution_count":33},{"id":"4f77b5b6-caaa-4765-905d-84a7f5a8acb7","cell_type":"code","source":"# evaluate mlp head \nper = evaluate_model(pr_model_mlp.to('cuda'), test_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:32:34.658152Z","iopub.execute_input":"2025-04-07T20:32:34.658546Z","iopub.status.idle":"2025-04-07T20:32:46.322337Z","shell.execute_reply.started":"2025-04-07T20:32:34.658514Z","shell.execute_reply":"2025-04-07T20:32:46.321022Z"}},"outputs":[{"name":"stdout","text":"PER on test dataset: 14.24%\n","output_type":"stream"}],"execution_count":34},{"id":"c55a2e6b-87d6-4290-9b76-bc1d612fc149","cell_type":"code","source":"pr_model_lstm = PhoneticRecognition(len(phonetics), use_lstm=True).to(DEVICE)\nlightning_model_lstm = PhoneticRecognitionModel(pr_model_lstm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:32:49.032977Z","iopub.execute_input":"2025-04-07T20:32:49.033366Z","iopub.status.idle":"2025-04-07T20:32:49.692479Z","shell.execute_reply.started":"2025-04-07T20:32:49.033331Z","shell.execute_reply":"2025-04-07T20:32:49.691661Z"}},"outputs":[{"name":"stderr","text":"Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":35},{"id":"c50902ed-be8e-4510-868d-e79c693c5749","cell_type":"code","source":"trainer = pl.Trainer(\n    max_epochs=10,\n    callbacks=[checkpoint_callback],\n    logger=logger,\n    accelerator='cuda' if torch.cuda.is_available() else 'cpu',\n    devices=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:33:13.146103Z","iopub.execute_input":"2025-04-07T20:33:13.146465Z","iopub.status.idle":"2025-04-07T20:33:13.191463Z","shell.execute_reply.started":"2025-04-07T20:33:13.146435Z","shell.execute_reply":"2025-04-07T20:33:13.190575Z"}},"outputs":[],"execution_count":39},{"id":"e4aad735-abb3-420f-8f67-b27625fd24c2","cell_type":"code","source":"trainer.fit(lightning_model_lstm, train_dataloader, val_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:33:14.021920Z","iopub.execute_input":"2025-04-07T20:33:14.022285Z","iopub.status.idle":"2025-04-07T20:39:49.700641Z","shell.execute_reply.started":"2025-04-07T20:33:14.022251Z","shell.execute_reply":"2025-04-07T20:39:49.699763Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68cf7aeb47a94a8488fc6dc845dc6bb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}],"execution_count":40},{"id":"d623a342-7d6c-40ee-bdf9-706f11b2885b","cell_type":"code","source":"# evaluate lstm head \nper = evaluate_model(pr_model_lstm.to(DEVICE), test_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T20:39:51.550268Z","iopub.execute_input":"2025-04-07T20:39:51.550831Z","iopub.status.idle":"2025-04-07T20:40:03.725163Z","shell.execute_reply.started":"2025-04-07T20:39:51.550779Z","shell.execute_reply":"2025-04-07T20:40:03.723850Z"}},"outputs":[{"name":"stdout","text":"PER on test dataset: 14.51%\n","output_type":"stream"}],"execution_count":41},{"id":"ba13d5aa-5dc5-4f02-8935-cf7e2565cc9e","cell_type":"markdown","source":"In our final evaluation, both the simple MLP head and the more complex LSTM head performed similarly, each achieving approximately a 14% error rate on the test set. Notably, both architectures started to overfit after around 10 epochs—even after increasing the dropout rate to 0.5 for both heads. Additionally, after testing the last hidden state as well as other intermediate layers, we found that using the 9th hidden state of Wav2Vec produced the best results for our phoneme recognition task.","metadata":{}},{"id":"23bae73b-919f-4d0a-8bd4-f0ca5676d05c","cell_type":"markdown","source":"Note: We also observed that the notebook interface does not display the loss values, so here are the recorded numbers for reference: for the MLP head, the validation loss was 0.243 and the epoch-level training loss was 0.148; for the LSTM head, the validation loss was 0.260 and the epoch-level training loss was 0.169.","metadata":{}}]}